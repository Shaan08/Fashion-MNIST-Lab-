{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "Adam learning rate: 0.005\n",
    "Gd learning rate: 0.5\n",
    "Decay: 0.96 every 500 iter\n",
    "\n",
    "Adam without dropout, without decay\n",
    "Final test accuracy 0.8927\n",
    "Final test loss 0.50648\n",
    "\n",
    "Adam without dropout, with decay\n",
    "Final test accuracy 0.897\n",
    "Final test loss 0.508396\n",
    "\n",
    "gd-no-deacy-no-reg\n",
    "Final test accuracy 0.1282\n",
    "Final test loss 2.214514\n",
    "\n",
    "adam-decay-reg\n",
    "Final test accuracy 0.8882\n",
    "Final test loss 0.354514\n",
    "\n",
    "Observations:\n",
    "\n",
    "Results show that AdamOptimizer was better than GD, and that AdamOptimizer with decay peformed best. We however did not see much change in the accuracy even after adding dropout which is slighly surprising since we see that there is some overfitting so it should be possible to increase the test accuracy a bit. when we added dropout, we got less overfitting but lesser accuracy as well.We use the SAME padding here and our stride here is 1, so the input will preserve the dimension. Thus in our case the input dimensions are transformed as:\n",
    "1x28x28 -> 4x28x28 -> 8x14x14 -> 12x7x7 -> 588 -> 10\n",
    "The downsampling is due to stride > 1, and the FCC flattens the dimensions and softmax squashes it to 10 probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# all tensorflow api is accessible through this\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# to visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "# 70k mnist dataset that comes with the tensorflow container\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Enable deterministic comparisons between executions\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "#Initializing values for the constants used below\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "NUM_CLASSES = 10\n",
    "NUM_HIDDEN_1 = 200\n",
    "NUM_HIDDEN_2 = 100\n",
    "NUM_HIDDEN_3 = 60\n",
    "NUM_HIDDEN_4 = 30\n",
    "DROPOUT_RATE = 0.5\n",
    "GD_LEARNING_RATE = 0.5\n",
    "ADAM_LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n",
      "y = 7 (Sneaker)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEn1JREFUeJzt3V2MlGWWB/D/EaFBaD6EhoA0Nq2wxk8mtmgyaNjATMRM\nxPHCDBeEjTrMBTtZkjFZ416sl2bjMPFiM5FZYXAzy8xGxojGuFFcJROR2BJWENkVTQPdQNNAy/c3\nZy/6ZdJgv+cU9VTVW835/xJCd516q56u5k99nPd5HlFVEFE8NxQ9ACIqBsNPFBTDTxQUw08UFMNP\nFBTDTxQUw08UFMNPFBTDTxTUjbW8swkTJmhLS0st77Imjhw5YtbPnj1r1ocNG2bWGxsbk46vV729\nvWbde1xHjhxp1keMGJFbGz16tHmsiJj1etXR0YFDhw6VNPik8IvIowBeATAEwL+p6kvW9VtaWtDe\n3p5yl7lST1P2ftnW7a9du9Y8dteuXWZ9+vTpZn3u3Llmvbm52azXq3Xr1pl173F94IEHzPo999yT\nW5s3b555bENDg1mvV21tbSVft+yX/SIyBMC/AlgA4E4Ai0TkznJvj4hqK+U9/2wAu1T1W1U9B+CP\nABZWZlhEVG0p4b8FwN5+33dml11BRJaKSLuItPf09CTcHRFVUtU/7VfVlarapqptTU1N1b47IipR\nSvi7APT/pGlqdhkRDQIp4f8MwAwRmS4iwwD8DMD6ygyLiKqt7Fafql4Qkb8H8F/oa/WtUtUvKzay\nax+PWb/hBvv/uRMnTpj1+fPn59ZaW1vNY4cPH27WP/nkE7P+9NNPm/ULFy7k1mbPnm0ea7XDAP8c\ng7ffftusW/f/zjvvmMc+/vjjZr2ry36huXPnztza8uXLzWM/+ugjsz5lyhSz7v17rIfzCJL6/Kr6\nLoB3KzQWIqohnt5LFBTDTxQUw08UFMNPFBTDTxQUw08UVE3n81eT18f3PPnkk2bdWofAm1fu8aZh\nzpw506xv3749t7Zt2zbz2EuXLpn122+/3awfPnzYrB84cCC3tmzZMvPYoUOHmnXv3IyxY8fm1ubM\nmWMeu2LFCrP+8ssvm/WUKeK1OgeAz/xEQTH8REEx/ERBMfxEQTH8REEx/ERBDapWX0p75L333iv7\ntgF7eeyTJ0+ax44ZM8ase+2yG2+0f00PP/xwbu2+++4zj/XGPm7cOLO+ePFisz5x4sTc2rFjx8xj\nT506Zdatqcxe/aabbjKP7ezsNOsbN24064888ohZr4cpvXzmJwqK4ScKiuEnCorhJwqK4ScKiuEn\nCorhJwpqUPX5U7z66qtmfdSoUWb9/PnzuTWvHz1+/Hiz7vV8rfsGgIMHD+bWrG2qAb/fffr0abPu\nLUve3d2dW/OmYXtTer3jrcfV+515256vXr3arHt9/nrAZ36ioBh+oqAYfqKgGH6ioBh+oqAYfqKg\nGH6ioJL6/CLSAeA4gIsALqiqvQZ1Iqtv683ttnrhANDc3GzWrSWuvXnpZ86cMesNDQ1JdWstAu8c\ngdR55d7PNmTIkNyad45AKut35vX5vbGdO3fOrJ89e9ase7/TWqjEST5/q6qHKnA7RFRDfNlPFFRq\n+BXAByLyuYgsrcSAiKg2Ul/2z1HVLhGZCOB9Edmpqlcsbpb9p7AUAKZNm5Z4d0RUKUnP/Kralf19\nEMCbAGYPcJ2Vqtqmqm1NTU0pd0dEFVR2+EVkpIg0Xv4awI8B5O8YSUR1JeVl/yQAb2atohsB/Ieq\n2utjE1HdKDv8qvotAHtR+BratGmTWe/t7TXrXp/f6od7ve6LFy+a9dRevFX31vy3+vCAf/6Et8W3\ndQ6C12v3fm5vPr9V9x4Xr37okN3d/vDDD836ggULzHotsNVHFBTDTxQUw08UFMNPFBTDTxQUw08U\n1HWzdPeKFSvMurdNtjcF02qJea06r13mHe8tn21ND/WmjnqtOm/qqjd2q93mLc3tTcP27tvaXtxb\nstxbutv7naxdu9ass9VHRIVh+ImCYviJgmL4iYJi+ImCYviJgmL4iYK6bvr8H3/8sVl/6KGHzPqJ\nEyfMutXn93rGXq/d6xl7vfiU6cZer91jTdkF7OnM3nRib3vxXbt2mfVJkybl1rzfmTcF3Pu5d+zY\nYdat36k3VblS+MxPFBTDTxQUw08UFMNPFBTDTxQUw08UFMNPFNSg6vN3dnbm1rzdgBobG836nj17\nyhoTkL5WgLdEtddTtuqpW3B75xik9KS92/Z+Z3v37jXr1pz8+fPnm8d66xh4y463tLSYdWvst956\nq3lspfCZnygohp8oKIafKCiGnygohp8oKIafKCiGnygot88vIqsA/ATAQVW9O7vsZgB/AtACoAPA\nU6pqT4CuAGsdd2/ut9fvHjVqlFn/7rvvcmvefH1vu2dvzr3X57f2BfDmzHvbh3u8sVnnAXh9/tTf\naXd3d27tm2++MY/15vt7ezF4Y9u4cWNubfHixeaxlVLKM//vATx61WXPA9igqjMAbMi+J6JBxA2/\nqm4EcOSqixcCWJN9vQbAExUeFxFVWbnv+Sep6v7s6wMA8tdLIqK6lPyBn/a96ct94yciS0WkXUTa\ne3p6Uu+OiCqk3PB3i8hkAMj+zv0kTlVXqmqbqrZ5k2+IqHbKDf96AEuyr5cAeKsywyGiWnHDLyJr\nAWwC8Dci0ikizwB4CcCPRORrAPOz74loEHH7/Kq6KKc0r8Jjca1fvz63duTI1Q2JK508edKsez3n\no0eP5ta8XveUKVPMujc33OsZp6yN7/EeF+9nT3H+/HmzPnr0aLNu9er37dtnHjtt2jSzfvjw4bLv\nGwA2b96cW6unPj8RXYcYfqKgGH6ioBh+oqAYfqKgGH6ioAbV0t3PPvtsbm337t3msd5W1Js2bTLr\n1lLO48aNM4+12oRA+tLdVqvPOzaljQj4rUSrVegtjz127Niybxuwp1qPHz/ePNaaDgwA999/v1l/\n8MEHzfpzzz1n1muBz/xEQTH8REEx/ERBMfxEQTH8REEx/ERBMfxEQQ2qPv/UqVNza6tXr67hSK5k\nLesNAHPnzjXrd911l1n3lom2evlen9/rlXt9/pQtwL0lzb26tQU3YG+T/cYbb5jHRsBnfqKgGH6i\noBh+oqAYfqKgGH6ioBh+oqAYfqKgBlWf3+pZp86J91i339nZaR7rzVv3eukpvXavj586n9/agtvj\nLc09fPhws37HHXeYdW/57BSp6ySk/FuuFD7zEwXF8BMFxfATBcXwEwXF8BMFxfATBcXwEwXl9vlF\nZBWAnwA4qKp3Z5e9CODnAHqyq72gqu9Wa5C1kNK37e3tNY9tbW0ta0yXeb12b967JXULbm9sFu8c\ngdOnT5v1iRMnmvXGxsZrHlOlpJ4HUAulPPP/HsCjA1z+G1Wdlf0Z1MEnisgNv6puBHCkBmMhohpK\nec//SxH5QkRWiYi9XxUR1Z1yw/9bAK0AZgHYD+DXeVcUkaUi0i4i7T09PXlXI6IaKyv8qtqtqhdV\n9RKA3wGYbVx3paq2qWpbU1NTueMkogorK/wiMrnftz8FsL0ywyGiWiml1bcWwFwAE0SkE8A/A5gr\nIrMAKIAOAL+o4hiJqArc8KvqogEufq0KYylUSt81da2A1OOHDBmSW0vt43uPS+p6AZajR4+ade/8\nhpS1Bqo5X79e8Aw/oqAYfqKgGH6ioBh+oqAYfqKgGH6ioAbV0t0pbaPUJapT2mlevZqq3YZMuf2h\nQ4eWfSzgL4k+fvz4sm87tZVXD1N2PXzmJwqK4ScKiuEnCorhJwqK4ScKiuEnCorhJwpqUPX5Lal9\n15Tpn94S06nnAaTUU/vN3uOaMrbUKbkpy4Z7qn1+RD2cB8BnfqKgGH6ioBh+oqAYfqKgGH6ioBh+\noqAYfqKgrps+f5Hzr7u6usx6yjkEgN8Pt8Ze9BLT1u17933q1Cmz7v1se/bsMevVVA99fA+f+YmC\nYviJgmL4iYJi+ImCYviJgmL4iYJi+ImCcvv8ItIM4HUAkwAogJWq+oqI3AzgTwBaAHQAeEpVe6s3\n1OpKOQ/A6/Nba/6XoqGhwaxb5xGk9pu9cxSquU5C6rkZx48fL/u+U8+PuF7m818A8CtVvRPAQwCW\nicidAJ4HsEFVZwDYkH1PRIOEG35V3a+qW7KvjwP4CsAtABYCWJNdbQ2AJ6o1SCKqvGt6TSYiLQB+\nAGAzgEmquj8rHUDf2wIiGiRKDr+IjAKwDsByVT3Wv6Z9b3AGfJMjIktFpF1E2nt6epIGS0SVU1L4\nRWQo+oL/B1X9c3Zxt4hMzuqTARwc6FhVXamqbara1tTUVIkxE1EFuOGXvo8lXwPwlaqu6FdaD2BJ\n9vUSAG9VfnhEVC2lTOn9IYDFALaJyNbsshcAvATgP0XkGQC7ATxVnSHWRkrrxXs747X6vHpK28lb\nWvvChQtl3zaQ1srzlt72btvb4jtli+4I3PCr6l8A5P3rm1fZ4RBRrfAMP6KgGH6ioBh+oqAYfqKg\nGH6ioBh+oqCum6W7i9TZ2WnWvX60tzS3x+rle318r9eeukW3dfzw4cPNY8+dO2fWvePPnDlTVq2U\n267m+Q+1Uv8jJKKqYPiJgmL4iYJi+ImCYviJgmL4iYJi+ImCYp+/Arx546lbTXvz/c+ePZtb8/rw\n3jkGqX1+q1fv3bb3c6eMbd++feaxra2tSfc9GPCZnygohp8oKIafKCiGnygohp8oKIafKCiGnyio\nMH3+1C2TrXnx3txwrxfuzak/f/582cdXc0+AUlhjT13nwKtb93369Gnz2Aj4zE8UFMNPFBTDTxQU\nw08UFMNPFBTDTxQUw08UlNvnF5FmAK8DmARAAaxU1VdE5EUAPwdweXP6F1T13WoNtJ55ffqGhoak\n47014K1evdfH9+7b68V75wEMGzbMrKfcd8p5AtfDfPxUpZzkcwHAr1R1i4g0AvhcRN7Par9R1Zer\nNzwiqhY3/Kq6H8D+7OvjIvIVgFuqPTAiqq5res8vIi0AfgBgc3bRL0XkCxFZJSLjco5ZKiLtItLe\n09Mz0FWIqAAlh19ERgFYB2C5qh4D8FsArQBmoe+Vwa8HOk5VV6pqm6q2NTU1VWDIRFQJJYVfRIai\nL/h/UNU/A4CqdqvqRVW9BOB3AGZXb5hEVGlu+KXv4+LXAHylqiv6XT6539V+CmB75YdHRNVSyqf9\nPwSwGMA2EdmaXfYCgEUiMgt97b8OAL+oyghLVO3WjTU19tNPPzWPnTFjhln3WoHeMtMTJkzIrY0Y\nMcI81mv1eVtVHz582KwfPXo0t+a16jy33XabWbem9I4bN+BHVCXzWqiDQSmf9v8FwEA/aciePtH1\ngmf4EQXF8BMFxfATBcXwEwXF8BMFxfATBXXdLN3t9V1T+7LW8Tt37jSP3bZtm1nv6uoy61u2bDHr\n1tLhXp/e6sMD/nkC9957r1m37r+5udk81jsdfMyYMWZ95syZZR/r8aZZDwaD/ycgorIw/ERBMfxE\nQTH8REEx/ERBMfxEQTH8REFJLZcwFpEeALv7XTQBwKGaDeDa1OvY6nVcAMdWrkqO7VZVLWm9vJqG\n/3t3LtKuqm2FDcBQr2Or13EBHFu5ihobX/YTBcXwEwVVdPhXFnz/lnodW72OC+DYylXI2Ap9z09E\nxSn6mZ+IClJI+EXkURH5XxHZJSLPFzGGPCLSISLbRGSriLQXPJZVInJQRLb3u+xmEXlfRL7O/k5b\ng7qyY3tRRLqyx26riDxW0NiaReS/RWSHiHwpIv+QXV7oY2eMq5DHreYv+0VkCID/A/AjAJ0APgOw\nSFV31HQgOUSkA0CbqhbeExaRRwCcAPC6qt6dXfYvAI6o6kvZf5zjVPUf62RsLwI4UfTOzdmGMpP7\n7ywN4AkAf4cCHztjXE+hgMetiGf+2QB2qeq3qnoOwB8BLCxgHHVPVTcCOHLVxQsBrMm+XoO+fzw1\nlzO2uqCq+1V1S/b1cQCXd5Yu9LEzxlWIIsJ/C4C9/b7vRH1t+a0APhCRz0VkadGDGcCkbNt0ADgA\nYFKRgxmAu3NzLV21s3TdPHbl7HhdafzA7/vmqOosAAsALMte3tYl7XvPVk/tmpJ2bq6VAXaW/qsi\nH7tyd7yutCLC3wWg/+JtU7PL6oKqdmV/HwTwJupv9+Huy5ukZn8fLHg8f1VPOzcPtLM06uCxq6cd\nr4sI/2cAZojIdBEZBuBnANYXMI7vEZGR2QcxEJGRAH6M+tt9eD2AJdnXSwC8VeBYrlAvOzfn7SyN\ngh+7utvxWlVr/gfAY+j7xP8bAP9UxBhyxtUK4H+yP18WPTYAa9H3MvA8+j4beQbAeAAbAHwN4AMA\nN9fR2P4dwDYAX6AvaJMLGtsc9L2k/wLA1uzPY0U/dsa4CnnceIYfUVD8wI8oKIafKCiGnygohp8o\nKIafKCiGnygohp8oKIafKKj/By3In8MJU7rTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1194e2208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1 (Trouser)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\no5doKIgyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122936f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples in dataset 55000\n",
      "Number of test examples in dataset 10000\n",
      "Training set (images) shape: (55000, 784)\n",
      "Training set (labels) shape: (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data\n",
    "tf.set_random_seed(0)\n",
    "# load data\n",
    "mnist = input_data.read_data_sets('data/fashion', one_hot=True)\n",
    "\n",
    "# Just used for personal reference\n",
    "label_dict = {\n",
    " 0: 'T-shirt-top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "#Checking to see if we got the right data set.\n",
    "# Get 28x28 image\n",
    "sample_1 = mnist.train.images[123].reshape(28,28)\n",
    "# Get corresponding integer label from one-hot encoded data\n",
    "sample_label_1 = np.where(mnist.train.labels[183] == 1)[0][0]\n",
    "# Plot sample\n",
    "print(\"y = {label_index} ({label})\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))\n",
    "plt.imshow(sample_1, cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# Sample 2\n",
    "\n",
    "# Get 28x28 image\n",
    "sample_2 = mnist.train.images[190].reshape(28,28)\n",
    "# Get corresponding integer label from one-hot encoded data\n",
    "sample_label_2 = np.where(mnist.train.labels[190] == 1)[0][0]\n",
    "# Plot sample\n",
    "print(\"y = {label_index} ({label})\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))\n",
    "plt.imshow(sample_2, cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "print('Number of train examples in dataset ' + str(len(mnist.train.labels)))\n",
    "print('Number of test examples in dataset ' + str(len(mnist.test.labels)))\n",
    "print(\"Training set (images) shape: {shape}\".format(shape= mnist.train.images.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=mnist.train.labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ../data/fashion/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ../data/fashion/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ../data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ../data/fashion/t10k-labels-idx1-ubyte.gz\n",
      "Number of train examples in dataset 60000\n",
      "Number of test examples in dataset 10000\n",
      "iter: 0\n",
      "iter: 100\n",
      "iter: 200\n",
      "iter: 300\n",
      "iter: 400\n",
      "iter: 500\n",
      "iter: 600\n",
      "iter: 700\n",
      "iter: 800\n",
      "iter: 900\n",
      "iter: 1000\n",
      "iter: 1100\n",
      "iter: 1200\n",
      "iter: 1300\n",
      "iter: 1400\n",
      "iter: 1500\n",
      "iter: 1600\n",
      "iter: 1700\n",
      "iter: 1800\n",
      "iter: 1900\n",
      "iter: 2000\n",
      "iter: 2100\n",
      "iter: 2200\n",
      "iter: 2300\n",
      "iter: 2400\n",
      "iter: 2500\n",
      "iter: 2600\n",
      "iter: 2700\n",
      "iter: 2800\n",
      "iter: 2900\n",
      "iter: 3000\n",
      "iter: 3100\n",
      "iter: 3200\n",
      "iter: 3300\n",
      "iter: 3400\n",
      "iter: 3500\n",
      "iter: 3600\n",
      "iter: 3700\n",
      "iter: 3800\n",
      "iter: 3900\n",
      "iter: 4000\n",
      "iter: 4100\n",
      "iter: 4200\n",
      "iter: 4300\n",
      "iter: 4400\n",
      "iter: 4500\n",
      "iter: 4600\n",
      "iter: 4700\n",
      "iter: 4800\n",
      "iter: 4900\n",
      "iter: 5000\n",
      "iter: 5100\n",
      "iter: 5200\n",
      "iter: 5300\n",
      "iter: 5400\n",
      "iter: 5500\n",
      "iter: 5600\n",
      "iter: 5700\n",
      "iter: 5800\n",
      "iter: 5900\n",
      "iter: 6000\n",
      "iter: 6100\n",
      "iter: 6200\n",
      "iter: 6300\n",
      "iter: 6400\n",
      "iter: 6500\n",
      "iter: 6600\n",
      "iter: 6700\n",
      "iter: 6800\n",
      "iter: 6900\n",
      "iter: 7000\n",
      "iter: 7100\n",
      "iter: 7200\n",
      "iter: 7300\n",
      "iter: 7400\n",
      "iter: 7500\n",
      "iter: 7600\n",
      "iter: 7700\n",
      "iter: 7800\n",
      "iter: 7900\n",
      "iter: 8000\n",
      "iter: 8100\n",
      "iter: 8200\n",
      "iter: 8300\n",
      "iter: 8400\n",
      "iter: 8500\n",
      "iter: 8600\n",
      "iter: 8700\n",
      "iter: 8800\n",
      "iter: 8900\n",
      "iter: 9000\n",
      "iter: 9100\n",
      "iter: 9200\n",
      "iter: 9300\n",
      "iter: 9400\n",
      "iter: 9500\n",
      "iter: 9600\n",
      "iter: 9700\n",
      "iter: 9800\n",
      "iter: 9900\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "mnist = input_data.read_data_sets('../data/fashion', one_hot=True, validation_size=0)\n",
    "\n",
    "print('Number of train examples in dataset ' + str(len(mnist.train.labels)))\n",
    "print('Number of test examples in dataset ' + str(len(mnist.test.labels)))\n",
    "\n",
    "# Define placeholders for input data and for input truth labels\n",
    "x = tf.reshape(tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, 1]),\n",
    "               [-1, IMAGE_PIXELS])  # training examples (just one color channel, i.e grayscale)\n",
    "y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])  # correct answers(labels)\n",
    "\n",
    "\n",
    "# Define variables for the parameters of the model: Weights and biases, random-initialization with gaussian dist.\n",
    "# For each convolutional layer specify patch size, input channels and output channels\n",
    "\n",
    "# Helper function for weight variables, random gaussian weight initialization with 0 mean.\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# Helper function for bias variables, random gaussian weight initialization with 0 mean.\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# Helper function for convolutional layers with specified input size, stride\n",
    "# conv2d performs 2D convolution on 4D input, if input is 5D one can use 3D convolution\n",
    "# conv2d essentially means that the patch is strided over 2d surface for all input channels.\n",
    "# SAME padding means that zero-padding will be used on borders when patch does not fit perfectly on surface.\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=stride, padding='SAME')\n",
    "\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1]) # [numImages, height, width, channels]\n",
    "\n",
    "# Layer 1 - Convolutional, 5x5 patch strided 1 pixel at a time over the input,\n",
    "# 1 input channel of size 28x28,\n",
    "# 4 output channels per feature, feature map size 28x28\n",
    "# output_height = in_height/strides[1] = 28/1 = 28\n",
    "# output_width = in_width/strides[2] = 28/1 = 28\n",
    "W_conv1 = weight_variable([5, 5, 1, 4])\n",
    "b_conv1 = bias_variable([4])\n",
    "\n",
    "# Layer 2 - Convolutional, 5x5 patch strided 2 pixels at a time over the input,\n",
    "# 4 input channels of size 28x28,\n",
    "# 8 output channels per feature, feature map size is 14x14\n",
    "# output_height = in_height/strides[1] = 28/2 = 14\n",
    "# output_width = in_width/strides[2] = 28/2 = 14\n",
    "W_conv2 = weight_variable([5, 5, 4, 8])\n",
    "b_conv2 = weight_variable([8])\n",
    "\n",
    "# Layer 3 - Convolutional, 5x5 patch strided 2 pixels at a time over the input,\n",
    "# 9 input channels of size 28x28,\n",
    "# 12 output channels per feature, feature map size is 7x7\n",
    "# output_height = in_height/strides[1] = 14/2 = 7\n",
    "# output_width = in_width/strides[2] = 14/2 = 7\n",
    "W_conv3 = weight_variable([4, 4, 8, 12])\n",
    "b_conv3 = weight_variable([12])\n",
    "\n",
    "# Layer 4 - Fully connected,\n",
    "# 12 input channels of size 7x7,\n",
    "# 200 output channels, flattened\n",
    "W_fc = weight_variable([7 * 7 * 12, 200])\n",
    "b_fc = weight_variable([200])\n",
    "\n",
    "# Activation layer - Softmax\n",
    "# 200 input channels\n",
    "# 10 output channels, one per class\n",
    "W_fc2 = weight_variable([200, NUM_CLASSES])\n",
    "b_fc2 = bias_variable([NUM_CLASSES])\n",
    "\n",
    "# 2. Define the model - compute predicitions\n",
    "\n",
    "# hidden layers with relu, 3 convolutional layer and 1 FC layer\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, [1, 1, 1, 1]) + b_conv1)\n",
    "#h_conv1 = tf.nn.dropout(h_conv1, keep_prob=DROPOUT_RATE)\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, [1, 2, 2, 1]) + b_conv2)\n",
    "#h_conv2 = tf.nn.dropout(h_conv2, keep_prob=DROPOUT_RATE)\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, [1, 2, 2, 1]) + b_conv3)\n",
    "#h_conv3 = tf.nn.dropout(h_conv3, keep_prob=DROPOUT_RATE)\n",
    "h_conv3_flat = tf.reshape(h_conv3, [-1, 7 * 7 * 12])\n",
    "\n",
    "# Hidden FC layer output\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc) + b_fc)\n",
    "h_fc1 = tf.nn.dropout(h_fc1, keep_prob=0.5)\n",
    "\n",
    "# Readout/Softmax layer\n",
    "\n",
    "# Compute the logits, aka the inverse of the sigmoid/softmax outputs for the softmax layer\n",
    "logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "# Define the loss, which is the loss between softmax of logits and the labels\n",
    "# Tensorflow performs softmax (the output activation) as part of the loss for efficiency\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits, name='xentropy'))\n",
    "\n",
    "# 4. Define the accuracy\n",
    "# Correct prediction is black/white, either the classification is correct or not\n",
    "# Accuracy is the ratio of correct predictions over wrong predictions\n",
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "# Exponential decay of learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = ADAM_LEARNING_RATE\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.96, staircase=True)\n",
    "\n",
    "# 5. Train with an Optimizer\n",
    "\n",
    "# task 1\n",
    "#train_step = tf.train.GradientDescentOptimizer(GD_LEARNING_RATE).minimize(cross_entropy_loss)\n",
    "\n",
    "# task 2\n",
    "#train_step = tf.train.AdamOptimizer(ADAM_LEARNING_RATE).minimize(cross_entropy_loss)\n",
    "\n",
    "# task 3\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss, global_step=global_step)\n",
    "\n",
    "# initialize and run start operation\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# Function representing a single iteration during training.\n",
    "# Returns a tuple of accuracy and loss statistics.\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # actual learning\n",
    "    # reading batches of 100 images with 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, feed_dict={x: batch_X, y_: batch_Y})\n",
    "\n",
    "    # evaluating model performance for printing purposes\n",
    "    # evaluation used to later visualize how well you did at a particular time in the training\n",
    "    train_a = []  # Array of training-accuracy for a single iteration\n",
    "    train_c = []  # Array of training-cost for a single iteration\n",
    "    test_a = []  # Array of test-accuracy for a single iteration\n",
    "    test_c = []  # Array of test-cost for a single iteration\n",
    "\n",
    "    # If stats for train-data should be updates, compute loss and accuracy for the batch and store it\n",
    "    if update_train_data:\n",
    "        train_acc, train_cos = sess.run([accuracy, cross_entropy_loss], feed_dict={x: batch_X, y_: batch_Y})\n",
    "        train_a.append(train_acc)\n",
    "        train_c.append(train_cos)\n",
    "\n",
    "    # If stats for test-data should be updates, compute loss and accuracy for the batch and store it\n",
    "    if update_test_data:\n",
    "        test_acc, test_cos = sess.run([accuracy, cross_entropy_loss],\n",
    "                                      feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "        test_a.append(test_acc)\n",
    "        test_c.append(test_cos)\n",
    "\n",
    "    return train_a, train_c, test_a, test_c\n",
    "\n",
    "\n",
    "# 6. Train and test the model, store the accuracy and loss per iteration\n",
    "\n",
    "train_accuracy = []\n",
    "train_cost = []\n",
    "test_accuracy = []\n",
    "test_cost = []\n",
    "\n",
    "NUM_TRAINING_ITER = 10000\n",
    "NUM_EPOCH_SIZE = 100\n",
    "for i in range(NUM_TRAINING_ITER):\n",
    "    test = False\n",
    "    if i % NUM_EPOCH_SIZE == 0:\n",
    "        test = True\n",
    "        print(\"iter: \" + str(i))\n",
    "    a, c, ta, tc = training_step(i, test, test)\n",
    "    train_accuracy += a\n",
    "    train_cost += c\n",
    "    test_accuracy += ta\n",
    "    test_cost += tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
